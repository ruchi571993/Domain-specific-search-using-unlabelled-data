{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5VcHOInjGRg"
      },
      "source": [
        "The notebook aims at answering questions using LFQA with pretrained and domain adapated retriever and thereby comparing search results \n",
        "\n",
        "Domain adapted retriever model outperforms base model for most of the questions\n",
        "\n",
        "Note- The notebook does not involve code for domain adaptation using GPL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La0Oegi07gEY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0X1-GX8egqb"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-05T19:37:50.925607Z",
          "iopub.status.busy": "2022-09-05T19:37:50.924579Z",
          "iopub.status.idle": "2022-09-05T19:38:02.041334Z",
          "shell.execute_reply": "2022-09-05T19:38:02.040248Z",
          "shell.execute_reply.started": "2022-09-05T19:37:50.925568Z"
        },
        "id": "sUMwofHFzj6H"
      },
      "outputs": [],
      "source": [
        "!pip install -U pinecone-client==2.0.10\n",
        "!pip install opendatasets\n",
        "!pip install sentence_transformers datasets\n",
        "!pip install -U 'farm-haystack[pinecone]'==1.3.0\n",
        "!pip install lxml==4.9.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-05T19:38:12.489282Z",
          "iopub.status.busy": "2022-09-05T19:38:12.488939Z",
          "iopub.status.idle": "2022-09-05T19:38:12.509037Z",
          "shell.execute_reply": "2022-09-05T19:38:12.507938Z",
          "shell.execute_reply.started": "2022-09-05T19:38:12.489265Z"
        },
        "id": "UrUVjXVCjeoO"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import tqdm\n",
        "import random\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import pinecone\n",
        "import opendatasets as od\n",
        "import torch\n",
        "from haystack import Document\n",
        "from haystack.generator.transformers import Seq2SeqGenerator\n",
        "from haystack.utils import print_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deccTNX-ebq7"
      },
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpcVkiUaeP66"
      },
      "source": [
        "Unzip domain adapted model file present in Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-05T19:38:12.522175Z",
          "iopub.status.busy": "2022-09-05T19:38:12.521956Z",
          "iopub.status.idle": "2022-09-05T19:38:12.527535Z",
          "shell.execute_reply": "2022-09-05T19:38:12.526029Z",
          "shell.execute_reply.started": "2022-09-05T19:38:12.522155Z"
        },
        "id": "fY7IDCsNel0G"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/model.zip\n",
        "MODEL_PATH = '/content/drive/MyDrive/biencoder-arxiv/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBHn6hvI4xuc"
      },
      "source": [
        "We load \n",
        "\n",
        "(old_model) pretrained model, a state-of-the-art model trained on MS MARCO.\n",
        "\n",
        "(new_model)domain adapted model (biencoder-arxiv) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-05T19:38:19.669051Z",
          "iopub.status.busy": "2022-09-05T19:38:19.668629Z",
          "iopub.status.idle": "2022-09-05T19:38:21.467195Z",
          "shell.execute_reply": "2022-09-05T19:38:21.466274Z",
          "shell.execute_reply.started": "2022-09-05T19:38:19.669023Z"
        },
        "id": "kv3-0UNbgAb0"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "max_seq_length = 256\n",
        "old_model_name = \"msmarco-distilbert-base-tas-b\"\n",
        "old_model = SentenceTransformer(old_model_name)\n",
        "\n",
        "new_model_name = MODEL_PATH\n",
        "new_model = SentenceTransformer(new_model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxNJgFhflYiQ"
      },
      "source": [
        "# PineCone Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYaUQ4Pt4mz0"
      },
      "source": [
        "Logging into Pinecone and creating index negative-mines if not present"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7I3zbXA8YSe"
      },
      "source": [
        "Uploading current documents to Pinecone in batch with a namespace to perform faster search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3wOM1Bj4Zoq"
      },
      "source": [
        "Function to query top 3 similar embeddings from Pinecone, we take the query and get the embeddings based on the model provided. For all the matches, we get the ID and the TEXT from the metadata. This is then convert to a result dictionary to be used for Haystack using the document store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypj4X3eneq1J"
      },
      "source": [
        "PineCone Wrapper to perform operations such as upload, query and delete for our needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-05T20:18:24.208328Z",
          "iopub.status.busy": "2022-09-05T20:18:24.207978Z",
          "iopub.status.idle": "2022-09-05T20:18:24.232226Z",
          "shell.execute_reply": "2022-09-05T20:18:24.231449Z",
          "shell.execute_reply.started": "2022-09-05T20:18:24.208303Z"
        },
        "id": "U-hrxLfcXvqU"
      },
      "outputs": [],
      "source": [
        "class PineconeWrapper():\n",
        "    def __init__(self, model, top_k=3, dimensions=768):\n",
        "        pinecone.init(api_key='###', environment='us-west1-gcp')\n",
        "        # create new mining index if does not exist\n",
        "        if 'negative-mines' not in pinecone.list_indexes():\n",
        "            pinecone.create_index(\n",
        "                'negative-mines', dimension=dimensions,\n",
        "                metric='dotproduct', pods=1, pod_type='p1'  # limit of pods=1 for free plan (more pods == faster mining)\n",
        "            )\n",
        "        # connect\n",
        "        self.index = pinecone.Index('negative-mines')\n",
        "        print(\"Index Stats: \", self.index)\n",
        "\n",
        "        self.batch_size = 16\n",
        "        self.dimension = dimensions\n",
        "        self.model = model\n",
        "        self.top_k = top_k\n",
        "\n",
        "\n",
        "    def upload_pinecone(self, haystack_docs, namespace):\n",
        "        docs = [haystack_docs[i]['content'] for i in range(len(haystack_docs))]\n",
        "\n",
        "        # doc_dir = [haystack_docs[i]['meta']['doc_dir'] for i in range(len(haystack_docs))]\n",
        "\n",
        "        print(\"\\nUPLOAD PINECONE START!*******!\")\n",
        "        docs_emb = self.model.encode(docs, convert_to_tensor=True, show_progress_bar=True)\n",
        "        print(f\"Document Embeddings Shape : {docs_emb.shape}\")\n",
        "\n",
        "        index_data = self.index.describe_index_stats()\n",
        "        print(f\"Index Data before Adding : {index_data}\")\n",
        "        totalVectorCount = int(index_data['totalVectorCount'])\n",
        "\n",
        "        for i in tqdm.tqdm(range(0, len(docs_emb), self.batch_size)):\n",
        "            i_end = min(i+self.batch_size, len(docs_emb))\n",
        "            batch_emb = docs_emb[i:i_end, :].tolist()\n",
        "            # batch_data = docs[i:i_end]\n",
        "            \n",
        "            # batch_metadata = [{\"text\": batch_data[i]} for i in range(0, len(batch_data))]\n",
        "            batch_metadata = [{\"text\": haystack_docs[j]['content'], \n",
        "                               \"doc_dir\": haystack_docs[j]['meta']['doc_dir']} \n",
        "                              for j in range(i, i_end)]\n",
        "\n",
        "            batch_ids = [str(x+totalVectorCount) for x in range(i, i_end)]\n",
        "            # print(f\"Batch ID : {batch_ids}, Batch MetaData : {batch_metadata}\")\n",
        "            # print(f\"Batch ID : {batch_ids}, Batch Embeddings : {batch_emb}\")\n",
        "            # print(f\"Batch ID : {batch_ids}, Batch Data : {batch_data}\")\n",
        "            # upload to index\n",
        "            upload_vectors = list(zip(batch_ids, batch_emb, batch_metadata))\n",
        "            print(f\"\\nBatch Upload Vectors : {upload_vectors}\\n\")\n",
        "            self.index.upsert(vectors=upload_vectors, namespace=namespace)\n",
        "        \n",
        "        index_data = self.index.describe_index_stats()\n",
        "        print(f\"Index Data after Adding : {index_data}\")\n",
        "        print(\"\\nUPLOAD PINECONE END!*******!\")\n",
        "\n",
        "    def delete_vectors(self, namespace=None):\n",
        "        if not namespace:\n",
        "            index_data = self.index.describe_index_stats()\n",
        "            print(f\"Index Data before deleting : {index_data}\")\n",
        "\n",
        "            to_delete = []\n",
        "            for i in range(12263, 12294):\n",
        "                to_delete.append(str(i))\n",
        "            print(f\"Index to Delete : {to_delete}\")\n",
        "            self.index.delete(ids = to_delete)\n",
        "\n",
        "            index_data = self.index.describe_index_stats()\n",
        "            print(f\"Index Data after deleting : {index_data}\")\n",
        "        else:\n",
        "            index_data = self.index.describe_index_stats()\n",
        "            print(f\"Index Data before deleting : {index_data}\")\n",
        "\n",
        "            self.index.delete(delete_all=True, namespace=namespace)\n",
        "\n",
        "            index_data = self.index.describe_index_stats()\n",
        "            print(f\"Index Data after deleting : {index_data}\")\n",
        "\n",
        "    def query_pinecone(self, query, namespace):\n",
        "        query_emb = self.model.encode(query).tolist()\n",
        "        # print(query_emb.shape)\n",
        "        # print(query_emb)\n",
        "        \n",
        "        # res = index.query([query_emb], top_k = 10)\n",
        "        if namespace:\n",
        "            res = self.index.query([query_emb], top_k = self.top_k, namespace=namespace, include_metadata=True)\n",
        "        else:\n",
        "            res = self.index.query([query_emb], top_k = self.top_k)\n",
        "        # print(f\"Pinecone Results : {res}\")\n",
        "        \n",
        "        # ids = [match.id for match in res['results'][0]['matches']]\n",
        "        # scores = [match.score for match in res['results'][0]['matches']]\n",
        "        \n",
        "        # print(ids)\n",
        "        # print(scores)\n",
        "        answers = []\n",
        "        \n",
        "        for match in res['results'][0]['matches']:\n",
        "            vector_id = int(match.id)\n",
        "            score = match.score\n",
        "            # text = corpus[vector_id]\n",
        "            try:\n",
        "                text = match['metadata']['text']\n",
        "                doc_dir = match['metadata']['doc_dir']\n",
        "            except:\n",
        "                text = abstracts[vector_id]\n",
        "                doc_dir = \"NA\"\n",
        "                \n",
        "            result_dict = {\n",
        "                \"content\" : text,\n",
        "                \"context-type\" : \"text\",\n",
        "                \"meta\":{\n",
        "                    \"id\" : vector_id,\n",
        "                    \"score\": score,\n",
        "                    \"doc_dir\" : doc_dir\n",
        "                }\n",
        "            }\n",
        "            answers.append(result_dict)\n",
        "            # print(f\"Score : {score}, ID : {vector_id}, TEXT : {text}\")\n",
        "        \n",
        "        return answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW5YI9bfM44F"
      },
      "source": [
        "# Integrating LFQA and Haystack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhMV6-zMe45M"
      },
      "source": [
        "# Get Answers from LFQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajlBYnTNEAEr"
      },
      "source": [
        "Custom function to retrieve documents from input file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-05T20:18:39.646565Z",
          "iopub.status.busy": "2022-09-05T20:18:39.645372Z",
          "iopub.status.idle": "2022-09-05T20:18:39.652581Z",
          "shell.execute_reply": "2022-09-05T20:18:39.651907Z",
          "shell.execute_reply.started": "2022-09-05T20:18:39.646542Z"
        },
        "id": "ZDR8xNcpM44J"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def get_doc(all_doc_dir):\n",
        "    threshold = 300\n",
        "    doc_results = []\n",
        "\n",
        "    print(f\"Documents Found : {os.listdir(all_doc_dir)}\")\n",
        "\n",
        "    for document in os.listdir(all_doc_dir):\n",
        "        doc_dir = os.path.join(all_doc_dir, document)\n",
        "        print(f\"Processing Document : {doc_dir}\")\n",
        "        text = Path(doc_dir).read_text().replace(\"\\n\", \" \")\n",
        "        text = text.replace('\"', \"\")\n",
        "        out = []\n",
        "        \n",
        "        for chunk in text.split('. '):\n",
        "            if out and len(chunk)+len(out[-1]) < threshold:\n",
        "                out[-1] += ' '+chunk+'.'\n",
        "            else:\n",
        "                out.append(chunk+'.')\n",
        "\n",
        "        \n",
        "        for doc in out:\n",
        "            result_dict = {\n",
        "                    \"content\" : doc,\n",
        "                    \"context-type\" : \"text\",\n",
        "                    \"meta\":{\n",
        "                        \"id\" : None,\n",
        "                        \"score\": 0,\n",
        "                        \"doc_dir\" : document\n",
        "                    }\n",
        "                }\n",
        "            doc_results.append(result_dict)\n",
        "\n",
        "    return doc_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjVcAaYs4P20"
      },
      "source": [
        "Specify query and document dicrectory to use the file. Then we get the list of texts from the document. Now we pass the model and the query to top 3 documents from the query_pinecone function. Combined result contains the document from the texts as well as top 3 documents from query_pinecone function. For all the documents in combined results we create a document store. Then we pass this document for the LFQA to obtain the answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-05T20:18:40.131182Z",
          "iopub.status.busy": "2022-09-05T20:18:40.130132Z",
          "iopub.status.idle": "2022-09-05T20:18:40.138240Z",
          "shell.execute_reply": "2022-09-05T20:18:40.137380Z",
          "shell.execute_reply.started": "2022-09-05T20:18:40.131153Z"
        },
        "id": "DNreeniYc4KX"
      },
      "outputs": [],
      "source": [
        "def get_answers_wrapper(query, doc_dir, model, namespace, top_k):\n",
        "    doc_results = get_doc(doc_dir)\n",
        "    # print(f\"DOC RESULTS : {doc_results}\")\n",
        "\n",
        "    pineconeWrapper = PineconeWrapper(model, top_k=top_k, dimensions = 768)\n",
        "\n",
        "    pineconeWrapper.upload_pinecone(doc_results, namespace)\n",
        "\n",
        "    biencoder_results = pineconeWrapper.query_pinecone(query, namespace)\n",
        "    document_store = []\n",
        "\n",
        "    data = {}\n",
        "    data['query'] = query\n",
        "    \n",
        "    for idx, doc in enumerate(biencoder_results):\n",
        "        i = str(idx)\n",
        "        data[\"top_\"+i+\"_content\"] = doc['content']\n",
        "        data[\"top_\"+i+\"_dotscore\"] = doc['meta']['score']\n",
        "        data[\"top_\"+i+\"_document\"] = doc['meta']['doc_dir']\n",
        "        document_store.append(Document(doc['content']))\n",
        "\n",
        "    print(f\"All Documents : {document_store}\")\n",
        "\n",
        "    generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")\n",
        "\n",
        "    result = generator.predict(\n",
        "        query=query,\n",
        "        documents = document_store,\n",
        "        top_k=1\n",
        "    )\n",
        "\n",
        "    print_answers(result, details=\"minimum\")\n",
        "    \n",
        "    answers = result['answers']\n",
        "    for answer in answers:\n",
        "        final_ans = answer.answer\n",
        "        ans_score = answer.score\n",
        "        break\n",
        "    \n",
        "    data['answer'] = final_ans\n",
        "    data['score'] = ans_score\n",
        "    # return final_ans\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-05T20:18:41.042642Z",
          "iopub.status.busy": "2022-09-05T20:18:41.041432Z",
          "iopub.status.idle": "2022-09-05T20:18:41.046907Z",
          "shell.execute_reply": "2022-09-05T20:18:41.046317Z",
          "shell.execute_reply.started": "2022-09-05T20:18:41.042613Z"
        },
        "id": "zygLF6l5M44L"
      },
      "outputs": [],
      "source": [
        "def run_inference_refactored(query, doc_dir, model, top_k=5):\n",
        "    pineconeWrapper = PineconeWrapper(model, top_k=top_k)\n",
        "    pineconeWrapper.delete_vectors(namespace='Space_Search')\n",
        "    data = get_answers_wrapper(query, doc_dir, model, namespace='Space_Search', top_k=top_k)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSV Export"
      ],
      "metadata": {
        "id": "M1dQe22bq-fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_csv(queries, all_doc_dir):\n",
        "    all_data = []\n",
        "    for query in queries:\n",
        "        answers_new = run_inference_refactored(query, all_doc_dir, new_model, top_k=3)\n",
        "        answers_new['model_type'] = 'domain_adapted'\n",
        "        all_data.append(answers_new)\n",
        "\n",
        "        answers_old = run_inference_refactored(query, all_doc_dir, old_model, top_k=3)\n",
        "        answers_old['model_type'] = 'pretrained'\n",
        "        all_data.append(answers_old)\n",
        "    \n",
        "    df = pd.DataFrame(all_data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "ZxunjHc3orTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries=[\"What were the possible reasons for Hydrogen leak?\",\n",
        "    \"What are the next steps taken after the launch was called off?\",\n",
        "    \"Why is orion not planned to carry crews in the initial mission?\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "C74ecTHyYDgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs_dir='/content/drive/MyDrive/data/'\n",
        "df = get_csv(queries, all_docs_dir)\n",
        "df.to_csv(\"Result.csv\", index=None)"
      ],
      "metadata": {
        "id": "9Cju-9D8tMDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}